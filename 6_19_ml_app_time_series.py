# -*- coding: utf-8 -*-
"""6.19- ML - APP - TIME SERIES.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ac_roMBi4JqzoDTLPqFps7x4DK7wIvvR

# **Demand Forecasting and Inventory Optimization**

Persistent Deployment: For a more permanent deployment, consider hosting your Streamlit app on a cloud platform like Heroku, AWS, GCP, or Azure. These platforms provide more robust and scalable hosting solutions.

## STEP 1.
"""

!pip install streamlit pandas numpy scikit-learn joblib matplotlib pyngrok

import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Generate dates
date_range = pd.date_range(start='2021-01-01', end='2023-12-31', freq='D')

# Generate fictitious sales data
np.random.seed(42)
sales = np.random.poisson(lam=100, size=len(date_range))

# Generate fictitious inventory data
inventory = np.random.randint(50, 150, size=len(date_range))

# Create a DataFrame
data = pd.DataFrame({
    'date': date_range,
    'sales': sales,
    'inventory': inventory
})

data.head()

# Add seasonal and trend components
data['month'] = data['date'].dt.month
data['year'] = data['date'].dt.year

# Add seasonality (sales peaks in December and July)
data['sales'] = data['sales'] + np.where(data['month'] == 12, 50, 0)
data['sales'] = data['sales'] + np.where(data['month'] == 7, 30, 0)

# Add trend (increasing sales each year)
data['sales'] = data['sales'] + (data['year'] - 2021) * 10

# Save the data to a CSV file
data.to_csv('fictitious_sales_data.csv', index=False)

data.head()

"""##Modeling"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load data
data = pd.read_csv('fictitious_sales_data.csv')

# Convert date to datetime type
data['date'] = pd.to_datetime(data['date'])

# Create additional features if necessary
data['day_of_week'] = data['date'].dt.dayofweek
data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)

# Select features and target
features = ['month', 'day_of_week', 'is_weekend', 'inventory']
target = 'sales'

X = data[features]
y = data[target]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Model Training with Performance Evaluation
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib
import matplotlib.pyplot as plt

# Train a Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)



"""## Make Prediction"""

# Make predictions on the test set
y_pred = model.predict(X_test_scaled)

# Evaluate model performance
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f'Mean Absolute Error (MAE): {mae:.2f}')
print(f'Mean Squared Error (MSE): {mse:.2f}')
print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')
print(f'R-squared (R²): {r2:.2f}')

# Save the trained model
joblib.dump(model, 'sales_forecast_model.pkl')

# Plot actual vs. predicted values
plt.figure(figsize=(10, 6))
plt.plot(y_test.values, label='Actual Sales')
plt.plot(y_pred, label='Predicted Sales')
plt.legend()
plt.xlabel('Sample Index')
plt.ylabel('Sales')
plt.title('Actual vs Predicted Sales')
plt.show()

# Hyperparameter Tuning with Grid Search
from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the model
rf = RandomForestRegressor(random_state=42)

# Initialize Grid Search
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

# Fit Grid Search
grid_search.fit(X_train_scaled, y_train)

# Get the best parameters and model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Evaluate the best model
y_pred_best = best_model.predict(X_test_scaled)
mae_best = mean_absolute_error(y_test, y_pred_best)
mse_best = mean_squared_error(y_test, y_pred_best)
rmse_best = np.sqrt(mse_best)
r2_best = r2_score(y_test, y_pred_best)

print(f'Best Model Parameters: {best_params}')
print(f'Best Model Mean Absolute Error (MAE): {mae_best:.2f}')
print(f'Best Model Mean Squared Error (MSE): {mse_best:.2f}')
print(f'Best Model Root Mean Squared Error (RMSE): {rmse_best:.2f}')
print(f'Best Model R-squared (R²): {r2_best:.2f}')

"""#Creating the app"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile streamlit_app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# from prophet import Prophet
# import matplotlib.pyplot as plt
# from io import BytesIO
# 
# def convert_df_to_csv(df):
#     return df.to_csv(index=False).encode('utf-8')
# 
# st.title('Demand Forecasting and Inventory Optimization with Prophet')
# 
# st.write("""
# ## Instructions
# 1. Prepare a CSV file with the following columns:
#    - **date**: The date of the record (e.g., YYYY-MM-DD).
#    - **sales**: The actual sales figures for each date.
#    - **inventory**: The inventory levels for each date.
# 2. Upload the CSV file using the file uploader below.
# 3. View the predictions and analysis.
# """)
# 
# uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
# if uploaded_file is not None:
#     input_data = pd.read_csv(uploaded_file)
#     st.write("Uploaded Data:")
#     st.write(input_data.head())
#     required_columns = ['date', 'sales', 'inventory']
#     if all(column in input_data.columns for column in required_columns):
#         input_data['date'] = pd.to_datetime(input_data['date'])
#         prophet_data = input_data[['date', 'sales']].rename(columns={'date': 'ds', 'sales': 'y'})
#         model = Prophet()
#         model.fit(prophet_data)
#         st.write("## Forecast Future Demand")
#         forecast_period = st.selectbox("Select forecast period", ["3 months", "6 months", "1 year"])
#         if forecast_period == "3 months":
#             future_days = 90
#         elif forecast_period == "6 months":
#             future_days = 180
#         else:
#             future_days = 365
#         future_dates = model.make_future_dataframe(periods=future_days)
#         forecast = model.predict(future_dates)
#         forecast = forecast.rename(columns={
#             'ds': 'Date',
#             'yhat': 'Predicted Sales',
#             'yhat_lower': 'Prediction Interval Lower Bound',
#             'yhat_upper': 'Prediction Interval Upper Bound'
#         })
#         st.write("Future Predictions:")
#         st.write(forecast[['Date', 'Predicted Sales', 'Prediction Interval Lower Bound', 'Prediction Interval Upper Bound']].tail(future_days))
#         st.write("Visualization of Future Predictions:")
#         fig, ax = plt.subplots(figsize=(12, 6))
#         ax.plot(input_data['date'], input_data['sales'], label='Actual Sales', color='blue')
#         ax.plot(forecast['Date'], forecast['Predicted Sales'], label='Predicted Sales', color='orange')
#         ax.fill_between(forecast['Date'], forecast['Prediction Interval Lower Bound'], forecast['Prediction Interval Upper Bound'], color='orange', alpha=0.3)
#         ax.set_xlabel('Date')
#         ax.set_ylabel('Sales')
#         ax.set_title('Actual vs Predicted Sales with Prediction Intervals')
#         ax.legend()
#         ax.grid(True)
#         st.pyplot(fig)
#         st.write("## Download Predictions")
#         csv = convert_df_to_csv(forecast[['Date', 'Predicted Sales', 'Prediction Interval Lower Bound', 'Prediction Interval Upper Bound']])
#         st.download_button(
#             label="Download predictions as CSV",
#             data=csv,
#             file_name='forecast_predictions.csv',
#             mime='text/csv',
#         )
#     else:
#         st.write(f"Error: The uploaded file must contain the following columns: {', '.join(required_columns)}")

!ngrok authtoken 2i4MeUBPBRV3nSGceKXddDes9CR_2YX3xknWTWHCbBKD43kUw

!streamlit run streamlit_app.py --server.port 8501 &>/dev/null&
from pyngrok import ngrok

# Create tunnel to the Streamlit app
public_url = ngrok.connect(8501)
public_url

